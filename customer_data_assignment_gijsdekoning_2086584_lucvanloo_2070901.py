# -*- coding: utf-8 -*-
"""Customer_Data_assignment_GijsdeKoning-2086584_LucvanLoo-2070901.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/180Y8MNPK71Byd6xKtv7XVGhY8uTJWsA0

# Analysis of Customer Data - Project Assignment
#### Gijs de Koning (2086584)
#### Luc van Loo (2070901)


This assignment is devoted to the configuration, evaluation and the comparison of recommender system models. We will be using the Movielens Dataset created by GroupLens Research. 

This report is divided in 4 parts:
1. Exploring the dataset;
2. Preparing the data;
3. Building the algorithms;
4. Comparison against the baseline model.

## Dependencies
"""

import numpy as np
import pandas as pd
import seaborn as sns

sns.set()
sns.set_style('whitegrid')
sns.set_context('paper', font_scale=1.6, rc={"lines.linewidth": 2})
import matplotlib.pyplot as plt

from surprise import Dataset
from surprise import Reader
from surprise import NormalPredictor
from surprise.model_selection import cross_validate
import os

"""## Exploring the Dataset
While exploring the dataset we will give a basic overview of the data, some basic statistics and descriptive plots.

#### Loading the data
"""

data_dir = os.getcwd()

df = pd.read_csv(data_dir + "/ratings.csv")

df.head(50)

"""#### Basic overview
The basic overview shows that there is no missing data present in the dataset. This enables more precise models, because in theory we can use all of the 27.753.444 ratings collected for the 53.889 unique movies. However, using more than 27 million ratings will be to much too handle for our models later, so we will make a decision on which and how many ratings we will use based on the density and relative frequencies of our data.
"""

percent_missing = df.isnull().sum() * 100 / len(df)
unique_users = len(df['userId'].unique())
unique_anime = len(df['movieId'].unique())
total_ratings = len(df['rating'])

print('missing values: ' + str(round(percent_missing['rating'], 2)) + '%')
print('unique users: ' + str(unique_users))
print('unique movies: ' + str(unique_anime))
print('total ratings: ' + str(total_ratings))

"""#### Descriptive plots
When exploring the density of the ratings, there is a big spike in the first 5.000 to 10.000 movies. Furthermore, the density gets close to zero around the 12.000th movie id. This means most ratings were given to the first 10 to 12 thousand unique movies. For this reason we only plotted the relative frequency for the first 10.000 unique movie id's. 
"""

# Density plot
def plot_density(df):
    fig, ax = plt.subplots(figsize=(8,5.5))
    plot = sns.histplot(df['movieId'], kde=True, ax=ax)
    plot.set_ylim(0,300000)
plot_density(df)

"""


By further exploring the relative frequency we try to find the amount of top k-movies where the relative frequency comes close enough to zero, because the general rule of thumb states we try to use the amount of data/movies with a relative frequency close enough to zero. By inspecting the plot below we find that using the top 4000 movies is a reasonable choice. """

# Plot freq top 10.000 ratings

def plot_frequency(df, k):
    
    fig, ax = plt.subplots(figsize=(8,5.5))
    counts = df['movieId'].value_counts(sort=True, ascending=False)
    orig = counts.index.tolist()
    counts2 = counts.reset_index(inplace=False, drop=True)
    sub = counts2.index.tolist()

    sns.lineplot(x=counts2.index[0:k], y=counts2[0:k]/counts2[0:k].sum(), ax=ax)
    ax.fill_between(counts2.index[0:k], counts2[0:k]/counts2[0:k].sum(), alpha=0.5)
    ax.set_ylabel("relative frequency")
    ax.set_xlabel("top-k")

plot_frequency(df, k=10000)

"""To get a grasp on the distribution of the ratings over the entire dataset, we utilize the distribution plot below. It shows most given ratings fall between 3 and 5 while a rating of 4 is awarded the most with 28%. 0.5 has been awarded the least amount of times with a frequency of 2%.  """

# Distribution of ratings plot entire dataset

def dist_plot(df):
      fig = plt.figure()
      ax = fig.add_subplot(111)
      ax.set_ylabel("Frequency of ratings (%)")
      ax.set_xlabel("Ratings")
      ax.hist(df['rating'], edgecolor='black', weights=np.ones_like(df['rating']) / len(df['rating']))

dist_plot(df)

"""## Data selection
The function below selects the amount of top k-movies to be used. Even though we ideally would want to use the top 4.000 movies, experiments with this selection have shown two problems. First off, the notebook tries to allocate to much memory, which automattically shuts the notebook down. Additionally, finding the optimal hyperparameters and validating the model gets computationally expensive when using a larger selection. 

We tried running the grid search and the cross_validate function with larger selections (ranging from 1.000 to 4.000), but the notebook kept crashing. As a result the largest selection of top-k movies we have used is 750. 

We also make a distinction between the selection used for the grid search and the cross validation. The main reasoning behind this is to cut down on the computational complexity of the grid search by using a smaller selection of movies. 
"""

# Function to reduce computing time, by reducing sample size.  

def select(df, k, col):

    top_values = df[col].value_counts().nlargest(k)
    return df.loc[df[col].isin(top_values.index)]

df_mc = select(df, k = 750, col='movieId')
df_grid = select(df, k = 500, col='movieId')

"""The total amount of ratings and unique users are shown below to illustrate the difference in the selections. The main difference can be seen in the amount of total ratings which is around 2.7 million less for the selection with 500 movies. This difference causes a decrease in computational complexity which is necessary for the notebook to run the gridsearch in an acceptable timeframe and without allocating to much memory causing the notebook to crash. """

unique_users_grid = len(df_grid['userId'].unique())
unique_anime_grid = len(df_grid['movieId'].unique())
total_ratings_grid = len(df_grid["rating"])

print("Overview Grid Search selection:")
print('unique users: ' + str(unique_users_grid))
print('unique movies: ' + str(unique_anime_grid))
print('Total ratings: ' + str(total_ratings_grid))

unique_users_mc = len(df_mc['userId'].unique())
unique_anime_mc = len(df_mc['movieId'].unique())
total_ratings_mc = len(df_mc["rating"])

print()
print("Overview model comparison selection:")
print('unique users: ' + str(unique_users_mc))
print('unique movies: ' + str(unique_anime_mc))
print('Total ratings: ' + str(total_ratings_mc))

"""The plots below illustrate the distribution of both the selections. They show the distribution does not differ much from the distribution of the entire dataset. """

# Distribution of ratings plot selected dataset

def dist_plot_select(df, name):
      fig = plt.figure()
      ax = fig.add_subplot(111)
      ax.set_ylabel("Frequency of ratings (%)")
      ax.set_xlabel(name)
      ax.hist(df['rating'], edgecolor='black', weights=np.ones_like(df['rating']) / len(df['rating']))

dist_plot_select(df_mc, "Ratings MC")
dist_plot_select(df_grid, "Ratings GridSearch")

"""## Preparing the data
We use a reader with a rating scale of 0.5 trough 5 to prepare the data for the cross validation of the models and the grid search to find the best hyperparameters.


"""

# A reader is still needed but only the rating_scale param is required.
reader = Reader(rating_scale=(0.5, 5))

# The columns must correspond to user id, item id and ratings (in that order).
data = Dataset.load_from_df(df_mc[['userId', 'movieId', 'rating']], reader)

# A reader is still needed but only the rating_scale param is required.
reader = Reader(rating_scale=(0.5, 5))

# The columns must correspond to user id, item id and ratings (in that order).
data_grid = Dataset.load_from_df(df_grid[['userId', 'movieId', 'rating']], reader)

"""# Validating baseline model

The cross validation of the baseline model below shows the mean of the Root Mean Square Error (RMSE) over all folds on the test set of 1.3723. The mean RMSE of all folds on the train set is 1.3720. This indicates there is no overfitting problem.
"""

cross_validate(NormalPredictor(), data, cv=5,verbose=3, return_train_measures=True)

"""# The recommender systems
The chosen recommender systems are the KNNWithZScore and SVD algorithms. 

Here we provide the reasoning for why we have chosen these specific models, which hyperparameters of the algorithms will be tuned with the grid search and we discuss the results of the grid search. Additionally, we will report the results of the cross valildation with the chosen settings of the hyperparameters and compare these to the results of the baseline model. 

## Matrix factorization-based method

### Why normal SVD

Singular value decomposition (SVD) is a matrix factorization technique, which reduces the number of features of a dataset by reducing the amount of dimensions. It decompeses the rating matrix into the product of two matrices P and Q with latent features. So it uses a matrix structure where each column represents an item and each row a user. The elements of this matrix are the ratings that are given to items by users. In the context of the recommender system, it falls under the model based collaborative filtering methods (Kumar, March 2020).

Other matrix factorization techniques include SVD ++ and Non-negative Matrix Factorization (NMF). SVD ++ is an extension of SVD which takes into account implicit ratings. This is not interesting for our recommender system, because the Movielens dataset only includes explicit ratings. NMF is an alternative method that constrains the low rank matrixes to have non-negative entries. In A Comparative Study of Collaborative Filtering Algorithms  (Lee, Sun & Lebanon, May 2012) it was determined that in almost all cases the SVD algorithm outperformed the NMF model. On this basis the normal SVD algorithm was used for the recommender system.

#### Hyperparameters

The SVD algorithm has multiple hyperparameters that can be tuned. These include number of factors, the number of epochs to run, the learning rate and the regularization term for all parameters. For our model, the hyperparameters to be optimized are the number of epochs, the learning rate and the regularization term. 

##### Number of epochs
The SVD model uses a stochastic gradient descent procedure to minimize the RMSE. The number of epochs determines how many times the minimization is performed. This is crucial for increasing the performance of the model. However, it is important to watch out for overfitting and the computational complexity if we use to many epochs. 

##### Learning rate
Different learning rates determine at what pace the stochastic gradient descent algorithm, on which the SVD model searches for the minimum error value, looks for the local minimum. With lower learning rates there is a higher chance of finding the local minimum, however it will take more steps to reach it. With high learning rates there is a chance of missing the local minimum, but when it does find it will take less time to do so.  

##### Regularization term
The regularization term is important for the bias-variance trade-off. To much bias leads to overfitting, while to much variance leads to underfitting. By tuning the regularization term, we can fix this problem.

### Tuning the model
The GridSearch will look at the combinations of the following settings of the chosen hyperparameters:
- Number of epochs: 5, 10, 15
- Learning rate: 0.002, 0.005
- Regularization term: 0.02, 0.2

The default number of epochs in a SVD algorithm is 20. Because we expected this to be computationally to expensive, we decided to examine 5, 10 and 15 epochs and what impact it has on the performance.  

For the learning rate we included the default setting of 0.005 and an extra setting 0.002 to see the impact the different learning rates have.

We chose regularization terms of 0.02 and 0.2 to examine the effect of the default setting of the algorithm (0.02) and a different setting (0.2) on the possible overfitting problem of the model.


"""

from surprise.prediction_algorithms import SVD 
from surprise.model_selection import GridSearchCV

param_grid = {"n_epochs": [5, 10, 15], "lr_all": [0.002, 0.005], "reg_all": [0.02, 0.2]}
model_SVD = GridSearchCV(SVD, param_grid, measures=["rmse"], cv=5, joblib_verbose = 1)

model_SVD.fit(data_grid)

# Mean results 
results_df = pd.DataFrame.from_dict(model_SVD.cv_results)
print(results_df[["mean_test_rmse", "mean_fit_time", "params"]])

# best RMSE score
print(model_SVD.best_score["rmse"])

# combination of parameters that gave the best RMSE score
print(model_SVD.best_params["rmse"])

"""### Chosen hyperparameters

In all configurations a learning rate of 0.005 with a regularization term of 0.02 resulted in the best RMSE. We also tried a manual configuration with a learning rate of 0.01, but the performance of this experiment was worse. While examining the difference in amount of epochs, it is apparent the RMSE becomes lower with more epochs. However, this difference in RMSE is not significant enough while taking the time it takes to fit the model with more epochs in account. To keep the computational complexity to train the model acceptable, we choose to train the model for 10 epochs instead of 15. 

### Evaluating the model

The mean RMSE over all folds on the test set is 0.8333 and on the training set 0.7918. This could indicate a small overfitting problem. The cross validation also reports a mean time it takes to model to fit to the data of 58 seconds.

Furthermore, the RMSE and time it takes to fit the model are consistent over all 5 folds of the cross validation.

"""

model_SVD = SVD(n_epochs = 10, lr_all = 0.005, reg_all = 0.02)

cross_validate(model_SVD, data, cv=5,verbose=3, return_train_measures=True)

"""### Comparison: Baseline model and SVD algorithm

Compared to the baseline model there is a definite increase in performance noticable. The baseline model has a mean RMSE of 1.3723 on the test data and 1.3724 on the train data, whereas the SVD algorithm has a mean RMSE of 0.8333 on the test set and 0.7918 on the training set. 

Compared to the baseline model the SVD algorithm does take a longer time to fit the data. The baseline model takes around 16 seconds to fit the model, while the SVD algorithm takes around 54 seconds. This difference is most likely the result of the number of epochs the SVD algorithm runs.

# KNN with Z-score

### Why KNN with Z-score
K-nearest neighbor (KNN) with Z-score is a collaborative filtering algorithm. We chose for a KNN algorithm as second model because it is well known as a good foundation for recommender system development (Bag et al., 2019). In our KNN model the Z-score normalization is taken into account to improve the algorithm accuracy (Imron, 2020). 

#### Hyperparameters
The KNN algorithm had a few hyperparameters that can be tuned. The (maximum) amount of neighbors, the minimum amount of neighbors and the similarity. 

##### k
The ‘k’ parameter is for the number of neighbors taken into account.

##### min_k
The ‘min_k’ parameter tells what the minimum number of neighbors is to take into account. 

##### sim_options
The ‘sim-options’ parameter is a dictionary with similarity options. In this dictionary the type of the similarities, the minimum support and the user-based parameters are specified.
The user-based parameter will tell if the similarity will be computed between items or between users. The min-support parameter will give the number of common items for the similarity. 

### Tuning the model
The GridSearch wil look at the combinations of the following hyperparameters:
-	(Maximum) Neighbors: 10, 20
-	Min_k: 1, 5
-	Similarity: 
    - Name: MSD, Cosine
    - Min_support: 1, 5
    - User_based: false

The default for min_k is 1, we have decided to test if the accuracy will increase with min_k: 5.

Cosine similarity is a similarity measure that is often used. Cosine similarity gives a higher accuracy according to papers (Singh et al., 2020), that’s why we used it in our GridSearch.
The similarity will be computed between items, because it works effective and will creates better suggestions (Ponnam et al., 2016).
"""

from surprise import KNNWithZScore

param_grid_KNN = {"k": [10, 20], "min_k": [1, 5], 'sim_options': {
        'name': ['msd', 'cosine'],
        'min_support': [1, 5],
        'user_based': [False],
    },
}
model_KNN = GridSearchCV(KNNWithZScore, param_grid_KNN, measures=["rmse"], cv = 5, joblib_verbose = 5)
model_KNN.fit(data_grid)

# Mean results 
results_df_knn = pd.DataFrame.from_dict(model_KNN.cv_results)
print(results_df_knn[["mean_test_rmse", "mean_fit_time", "params"]])

# best RMSE score
print(model_KNN.best_score["rmse"])

# combination of parameters that gave the best RMSE score
print(model_KNN.best_params["rmse"])

"""### Chosen hyperparameters
The GridSearch computed every possible option with the given options above. The best possible parameters are 10 neighbors, with a minimum of 5 neighbors. The Mean Squared Difference similarity performed best.

### Evaluating the model
The mean RMSE over all folds is 0.81768 on the test and 0.6596 on the trainset, which indicates a small overfitting problem unlike our SVD model.

"""

model_KNN_Z = KNNWithZScore(k = 10, min_k = 5, sim_options = {'name': 'msd', 'min_support': 1, 'user_based': False})

cross_validate(model_KNN_Z, data, cv=5,verbose=3, return_train_measures=True)

"""### Comparison: Baseline model and KNN algorithm
Compared to the baseline model there is an increase in performance noticable. The baseline model has a mean RMSE of 1.3723 on the test data and 1.3724 on the train data, whereas the KNNwithZscore algorithm has a mean RMSE of 0.81768 on the test set and 0.6596 on the training set. The KNNwithZscore algorithm takes longer to fit the model, the baseline model takes around 16 seconds and the KNNwithZscore takes around 31 seconds.

# References

Kumar. (2020, March 25). Singular Value Decomposition (SVD) & Its Application In Recommender System. Analytics India Magazine. Retrieved from https://analyticsindiamag.com/singular-value-decomposition-svd-application-recommender-system/

Lee, J., Sun, M., & Lebanon, G. (2012). A Comparative Study of Collaborative Filtering. Retrieved from https://arxiv.org/pdf/1205.3193.pdf

Bag, S., Ghadge, A., & Tiwaria, M. K. (2019, April). An integrated recommender system for improved accuracy and aggregate diversity. Retrieved 14 October 2022, from https://www.sciencedirect.com/science/article/pii/S0360835219301147

Vijay, H. (2022, February 9). Recommendation System using kNN. Auriga IT. Retrieved from https://www.aurigait.com/blog/recommendation-system-using-knn/

Singh, R. H., Maurya, S., Tripathi, T., Narula, T., & Srivastav, G. (2020, June). Movie Recommendation System using Cosine Similarity and KNN. researchgate.net. Retrieved from https://www.researchgate.net/profile/Gaurav-Srivastav-4/publication/344627182_Movie_Recommendation_System_using_Cosine_Similarity_and_KNN/links/5f856f49458515b7cf7c5ebf/Movie-Recommendation-System-using-Cosine-Similarity-and-KNN.pdf

Imron, M. A. (2020, September). Improving Algorithm Accuracy K-Nearest Neighbor Using Z-Score Normalization and Particle Swarm Optimization to Predict Customer Churn. Neliti. https://media.neliti.com/media/publications/327257-improving-algorithm-accuracy-k-nearest-n-7ae51630.pdf

Ponnam, L. T., Deepak Punyasamudram, S., Nallagulla, S. N., & Yellamati, S. (2016, February 1). Movie recommender system using item based collaborative filtering technique. IEEE Conference Publication | IEEE Xplore. Retrieved from https://ieeexplore.ieee.org/abstract/document/7602983
"""